#!/usr/bin/env python
#########################################################
#	Name: 	megafilelistgene			#
#	Description: Aspirateur de liens 		#
#########################################################


import urllib, re
import argparse

#Parse des arguments :
parser = argparse.ArgumentParser()
parser.add_argument('--url')
parser.add_argument('--megaupload')
parser.add_argument('--4shared')
args = parser.parse_args()

#REGEXP=r'href=[\'"]?(http://www.megaupload[^\'" >]+)'
#REGEXPMEGAUPLOAD=r' ?(http://www.megaupload[^\'" ><,]+)'
#REGEXP4SHARED=r'href=[\'"]?(http://www.4shared.com/file/[\'"<>]+)'

REGEXP='http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'


#-------------------------------------
# Description : aspirage de la page
#-------------------------------------
def dyson(url=None, regexp=None):
    url = args.url
    #Ouvrir l'url:
    f = urllib.urlopen(url)
    content = f.read()

    #trouver tous les liens : 
    match = re.findall(REGEXP,content)
    #Print urls : 
    for l in match :
        print(l)

url = args.url
dyson(url, REGEXP4SHARED)
